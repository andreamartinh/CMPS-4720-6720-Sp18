{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#I use the sklearn just to check how my model performed\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFile(fileLines):\n",
    "    temp1 = []\n",
    "    for line in fileLines:\n",
    "        temp2 = []\n",
    "        for element in line:\n",
    "            if element == ',':\n",
    "                continue\n",
    "            if element == '\\n' :\n",
    "                continue\n",
    "            if element == ' ' :\n",
    "                continue\n",
    "            temp2.append(int(element))\n",
    "        temp1.append(temp2)\n",
    "    return (temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "file_train = open('SPECT.train.csv', 'r') \n",
    "fileLinesTrain = file_train.readlines()\n",
    "dataset_train = cleanFile(fileLinesTrain)\n",
    "\n",
    "#load testing data\n",
    "file_test = open('SPECT.test.csv', 'r') \n",
    "fileLinesTest = file_test.readlines()\n",
    "dataset_test = cleanFile(fileLinesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) create probability table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_0no = [0]*23\n",
    "lst_0yes = [0]*23\n",
    "lst_1no = [0]*23\n",
    "lst_1yes = [0]*23\n",
    "for i in range(len(dataset_train)):\n",
    "    for j in range(len(dataset_train[0] )):\n",
    "        if dataset_train[i][j]==0 and dataset_train[i][0]==0 :\n",
    "            lst_0no[j] += 1\n",
    "        \n",
    "        if dataset_train[i][j]==0 and dataset_train[i][0]==1 :\n",
    "            lst_0yes[j] += 1\n",
    "        \n",
    "        \n",
    "        if dataset_train[i][j]==1 and dataset_train[i][0]==0 :\n",
    "            lst_1no[j] += 1\n",
    "        \n",
    "        if dataset_train[i][j]==1 and dataset_train[i][0]==1 :\n",
    "            lst_1yes[j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No = 0, Yes = 1\n",
    "lst_0no =np.array(lst_0no)\n",
    "lst_0yes =np.array(lst_0yes)\n",
    "\n",
    "lst_1no =np.array(lst_1no)\n",
    "lst_1yes =np.array(lst_1yes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40 was specified on the data specifications\n",
    "prob0no = (lst_0no)/40.0\n",
    "prob0yes = (lst_0yes)/40.0\n",
    "prob1no = (lst_1no)/40.0\n",
    "prob1yes = (lst_1yes)/40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob0no = np.delete(prob0no, 0)\n",
    "prob0yes = np.delete(prob0yes, 0)\n",
    "prob1no = np.delete(prob1no, 0)\n",
    "prob1yes = np.delete(prob1yes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prob0no = probability of the feature being 0 and being diagnozed 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.725 0.925 0.85  0.9   0.75  0.925 0.875 0.9   0.9   0.8   0.925 0.875\n",
      " 0.875 0.9   0.975 0.975 1.    1.    0.875 0.875 0.9   0.825]\n",
      "[0.55  0.75  0.625 0.675 0.65  0.825 0.6   0.55  0.725 0.625 0.675 0.625\n",
      " 0.425 0.7   0.875 0.675 0.8   0.85  0.75  0.675 0.625 0.525]\n",
      "[0.275 0.075 0.15  0.1   0.25  0.075 0.125 0.1   0.1   0.2   0.075 0.125\n",
      " 0.125 0.1   0.025 0.025 0.    0.    0.125 0.125 0.1   0.175]\n",
      "[0.45  0.25  0.375 0.325 0.35  0.175 0.4   0.45  0.275 0.375 0.325 0.375\n",
      " 0.575 0.3   0.125 0.325 0.2   0.15  0.25  0.325 0.375 0.475]\n"
     ]
    }
   ],
   "source": [
    "#prob0no = probability of the feature being 0 and being diagnozed 0\n",
    "print(prob0no)\n",
    "\n",
    "#prob0yes = probability of the feature being 0 and being diagnozed 1\n",
    "print(prob0yes)\n",
    "\n",
    "#prob1no = probability of the feature being 1 and being diagnozed 0\n",
    "print(prob1no)\n",
    "\n",
    "#prob1yes = probability of the feature being 1 and being diagnozed 1\n",
    "print(prob1yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the output in an array\n",
    "Y_test = []\n",
    "for i in range(len(dataset_test)):\n",
    "    temp = dataset_test[i].pop(0)\n",
    "    Y_test.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict based on probability\n",
    "Y_pred = []\n",
    "for i in range(len(dataset_test)):\n",
    "    probYes = 1\n",
    "    probNo = 1\n",
    "    for j in range(len(dataset_test[0])):\n",
    "        if dataset_test[i][j]==0:\n",
    "            probYes = probYes*prob0yes[j]\n",
    "            probNo = probNo*prob0no[j]\n",
    "        if dataset_test[i][j]==1:\n",
    "            probYes = probYes*prob1yes[j]\n",
    "            probNo = probNo*prob1no[j]\n",
    "\n",
    "    if probYes > probNo:\n",
    "        Y_pred.append(1)\n",
    "    else: \n",
    "        Y_pred.append(0)\n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Report and accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7754010695187166"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.67      0.32        15\n",
      "          1       0.96      0.78      0.87       172\n",
      "\n",
      "avg / total       0.90      0.78      0.82       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
